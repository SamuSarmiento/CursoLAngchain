{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Langchain 101: Prompts\n",
    "\n",
    "La nueva forma de programar modelos es mediante **prompts**. Un **prompt** se refiere a la entrada de datos al modelo. Esta entrada se construye a menudo a partir de múltiples componentes. Un **PromptTemplate** es responsable de la construcción de esta entrada. LangChain proporciona varias clases y funciones para facilitar la construcción y el trabajo con prompts."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducción\n",
    "\n",
    "Esta sección contiene todo lo relacionado con los prompts. Un prompt es el valor que se pasa al Modelo de Lenguaje. Este valor puede ser una cadena (para LLMs) o una lista de mensajes (para Modelos de Chat).\n",
    "\n",
    "Los tipos de datos de estos prompts son bastante simples, pero su construcción es cualquier cosa menos eso. LangChain incluye:\n",
    "\n",
    "- Una interfaz estándar para prompts de cadena y prompts de mensaje.\n",
    "\n",
    "- Una interfaz estándar (para empezar) para plantillas de avisos de cadena y plantillas de avisos de mensaje\n",
    "\n",
    "- Example Selectors: métodos para insertar ejemplos en el prompt para el modelo de lenguaje a seguir\n",
    "\n",
    "- OutputParsers: métodos para insertar instrucciones en el prompt como el formato en el que el modelo de lenguaje debe mostrar la información, así como métodos para analizar esa cadena de salida en un formato.\n",
    "\n",
    "Disponen de documentación detallada sobre tipos específicos de *string prompts*, tipos específicos de *chat prompts*, *Example Selectors* y *OutputParsers*.\n",
    "\n",
    "A continuación, cubriremos una interfaz estándar para empezar a trabajar con prompts sencillos."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PromptTemplates\n",
    "\n",
    "Los *PromptTemplates* son responsables de construir un *prompt value* ( La clase que representa una entrada a un modelo. ). Estos *PromptTemplates* pueden hacer cosas como formatear, seleccionar ejemplos y más. A un alto nivel, estos son básicamente objetos que exponen un método `format_prompt` para construir un prompt. Debajo del capó, puede pasar CUALQUIER COSA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_prompt = PromptTemplate.from_template(\"tell me a joke about {subject}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_prompt = ChatPromptTemplate.from_template(\"tell me a joke about {subject}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_prompt_value = string_prompt.format_prompt(subject=\"soccer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_prompt_value = chat_prompt.format_prompt(subject=\"soccer\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `to_string`\n",
    "\n",
    "Esto es lo que se llama cuando se pasa a un LLM (que espera texto sin formato)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tell me a joke about soccer'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string_prompt_value.to_string()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human: tell me a joke about soccer'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_prompt_value.to_string()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `to_messages`\n",
    "\n",
    "Esto es lo que se llama cuando se pasa a un ChatModel (que espera una lista de mensajes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='tell me a joke about soccer', additional_kwargs={}, example=False)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string_prompt_value.to_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='tell me a joke about soccer', additional_kwargs={}, example=False)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_prompt_value.to_messages()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Templates: Primeros pasos\n",
    "\n",
    "En este tutorial, aprenderemos sobre:\n",
    "\n",
    "- Qué es una plantilla de prompts y por qué es necesaria\n",
    "\n",
    "- Cómo crear una plantilla\n",
    "\n",
    "- Cómo pasar algunos ejemplos a una plantilla\n",
    "\n",
    "- Cómo seleccionar ejemplos para una plantilla"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¿Qué es una Prompt Template?\n",
    "\n",
    "Una plantilla de aviso es una forma reproducible de generar un prompt. Contiene una cadena de texto (\"la plantilla\"), que puede recibir un conjunto de parámetros del usuario final y generar un prompt.\n",
    "\n",
    "La plantilla puede contener:\n",
    "\n",
    "- Instrucciones para el modelo lingüístico\n",
    "\n",
    "- Un conjunto de ejemplos para ayudar al modelo lingüístico a generar una mejor respuesta\n",
    "\n",
    "- Una pregunta al modelo lingüístico\n",
    "\n",
    "El siguiente fragmento de código contiene un ejemplo de Prompt Template:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nI want you to act as a naming consultant for new companies.\\nWhat is a good name for a company that makes colorful socks?\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "\n",
    "template = \"\"\"\n",
    "I want you to act as a naming consultant for new companies.\n",
    "What is a good name for a company that makes {product}?\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"product\"],\n",
    "    template=template,\n",
    ")\n",
    "prompt.format(product=\"colorful socks\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¿Cómo crear un Prompt Template?\n",
    "\n",
    "Con la clase `PromptTemplate` se pueden crear prompts sencillos. Las plantillas de prompts pueden tomar cualquier número de variables de entrada y pueden formatearse para generar un prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tell me a joke.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ejemplo de solicitud sin variables de entrada\n",
    "no_input_prompt = PromptTemplate(input_variables=[], template=\"Tell me a joke.\")\n",
    "no_input_prompt.format()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tell me a funny joke.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Un ejemplo con una variable de entrada\n",
    "one_input_prompt = PromptTemplate(input_variables=[\"adjective\"], template=\"Tell me a {adjective} joke.\")\n",
    "one_input_prompt.format(adjective=\"funny\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tell me a funny joke about chickens.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiple_input_prompt = PromptTemplate(\n",
    "    input_variables=[\"adjective\", \"content\"], \n",
    "    template=\"Tell me a {adjective} joke about {content}.\"\n",
    ")\n",
    "multiple_input_prompt.format(adjective=\"funny\", content=\"chickens\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si no deseas especificar `input_variables` manualmente, también puedes crear una `PromptTemplate` utilizando el método de clase `from_template`. `Langchain` deducirá automáticamente las `input_variables` basándose en la plantilla pasada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adjective', 'content']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = \"Tell me a {adjective} joke about {content}.\"\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(template)\n",
    "prompt_template.input_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tell me a funny joke about chickens.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template.format(adjective=\"funny\", content=\"chickens\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puedes crear plantillas personalizadas que formateen el prompt de la manera que quieras."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Template formats\n",
    "\n",
    "Por defecto, `PromptTemplate` tratará la plantilla proporcionada como una f-string de Python. Puedes especificar otro formato de plantilla mediante el argumento `template_format`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Jinja2\n",
      "  Using cached Jinja2-3.1.2-py3-none-any.whl (133 kB)\n",
      "Collecting MarkupSafe>=2.0\n",
      "  Downloading MarkupSafe-2.1.3-cp310-cp310-win_amd64.whl (17 kB)\n",
      "Installing collected packages: MarkupSafe, Jinja2\n",
      "Successfully installed Jinja2-3.1.2 MarkupSafe-2.1.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Asegúrate de que jinja2 está instalado antes de ejecutar esto\n",
    "%pip install -U Jinja2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tell me a funny joke about chickens'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jinja2_template = \"Tell me a {{ adjective }} joke about {{ content }}\"\n",
    "prompt_template = PromptTemplate.from_template(template=jinja2_template, template_format=\"jinja2\")\n",
    "\n",
    "prompt_template.format(adjective=\"funny\", content=\"chickens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actualmente, `PromptTemplate` sólo soporta los formatos de plantillas `jinja2` y `f-string`. Si hay algún otro formato de plantilla que te gustaría utilizar, puedes abrir una incidencia en la página de [Github](https://github.com/hwchase17/langchain/issues)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validación de la plantilla\n",
    "\n",
    "Por defecto, `PromptTemplate` validará el string de la plantilla comprobando si los `input_variables` coinciden con las variables definidas en la `plantilla`. Puede desactivar este comportamiento estableciendo `validate_template` en `False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"I am learning langchain because {reason}.\"\n",
    "\n",
    "prompt_template = PromptTemplate(template=template, \n",
    "                                 input_variables=[\"reason\", \"foo\"], # ValueError debido a variables adicionales\n",
    "                                 validate_template=False) # No error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serializar plantilla de prompt\n",
    "\n",
    "Puedes guardar el `PromptTemplate` en un archivo en tu sistema de archivos local. `langchain` deducirá automáticamente el formato de archivo a través del nombre de la extensión del archivo. Actualmente, `langchain` soporta guardar plantillas en archivos YAML y JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template.save(\"awesome_prompt.json\") # Guardar en archivo JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import load_prompt\n",
    "loaded_prompt = load_prompt(\"awesome_prompt.json\")\n",
    "\n",
    "assert prompt_template == loaded_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`langchain` también soporta la carga de plantillas de prompts de LangChainHub, que contiene una colección de prompts útiles que puedes utilizar en tu proyecto. Puedes leer más sobre LangChainHub y los prompts disponibles [aquí](https://github.com/hwchase17/langchain-hub)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No `_type` key found, defaulting to `prompt`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\\n\\nCurrent conversation:\\n\\nHuman: What is 1 + 1?\\nAI:'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import load_prompt\n",
    "\n",
    "prompt = load_prompt(\"lc://prompts/conversation/prompt.json\")\n",
    "prompt.format(history=\"\", input=\"What is 1 + 1?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pasar *few shot* a una plantilla de prompts\n",
    "\n",
    "Los ejemplos de *few shot* son un conjunto de ejemplos que pueden utilizarse para ayudar al modelo lingüístico a generar una respuesta mejor.\n",
    "\n",
    "Para generar un prompt con ejemplos *few shot*, puedes utilizar `FewShotPromptTemplate`. Esta clase recibe un `PromptTemplate` y una lista de ejemplos *few shot*. Luego formatea la plantilla con los ejemplos.\n",
    "\n",
    "En este ejemplo, crearemos un prompt para generar antónimos de palabras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate, FewShotPromptTemplate\n",
    "\n",
    "# En primer lugar, elabora una lista con algunos ejemplos de tomas.\n",
    "examples = [\n",
    "    {\"word\": \"happy\", \"antonym\": \"sad\"},\n",
    "    {\"word\": \"tall\", \"antonym\": \"short\"},\n",
    "]\n",
    "\n",
    "# A continuación, especificamos la plantilla para dar formato a los ejemplos que hemos proporcionado.\n",
    "# Para ello utilizamos la clase `PromptTemplate`.\n",
    "example_formatter_template = \"\"\"Word: {word}\n",
    "Antonym: {antonym}\n",
    "\"\"\"\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"word\", \"antonym\"],\n",
    "    template=example_formatter_template,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Give the antonym of every input\n",
      "\n",
      "Word: happy\n",
      "Antonym: sad\n",
      "\n",
      "Word: tall\n",
      "Antonym: short\n",
      "\n",
      "Word: big\n",
      "Antonym: \n"
     ]
    }
   ],
   "source": [
    "# Por último, creamos el objeto `FewShotPromptTemplate`.\n",
    "few_shot_prompt = FewShotPromptTemplate(\n",
    "    # Estos son los ejemplos que queremos insertar en el prompt.\n",
    "    examples=examples,\n",
    "    # Así es como queremos formatear los ejemplos cuando los insertemos en el prompt.\n",
    "    example_prompt=example_prompt,\n",
    "    # El prefijo es un texto que va antes de los ejemplos en el prompt.\n",
    "    # Suele consistir en instrucciones.\n",
    "    prefix=\"Give the antonym of every input\\n\",\n",
    "    # El sufijo es algún texto que va después de los ejemplos en el prompt.\n",
    "    # Normalmente, aquí es donde irá la entrada del usuario\n",
    "    suffix=\"Word: {input}\\nAntonym: \",\n",
    "    # Las variables de entrada son las variables que espera el indicador general.\n",
    "    input_variables=[\"input\"],\n",
    "    # El example_selector es el string con la que uniremos el prefijo, los ejemplos y el sufijo.\n",
    "    example_separator=\"\\n\",\n",
    ")\n",
    "\n",
    "# Ahora podemos generar un prompt utilizando el método `format`.\n",
    "print(few_shot_prompt.format(input=\"big\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seleccionar ejemplos para una plantilla de prompts\n",
    "\n",
    "Si dispones de un gran número de ejemplos, puedes utilizar el `ExampleSelector` para seleccionar un subconjunto de ejemplos que sean más informativos para el Modelo de Lenguaje. Esto te ayudará a generar un prompt que tenga más probabilidades de generar una buena respuesta.\n",
    "\n",
    "A continuación, utilizaremos el `LengthBasedExampleSelector`, que selecciona ejemplos basados en la longitud de la entrada. Esto es útil cuando te preocupa construir un prompt que sobrepase la longitud de la ventana contextual. Para entradas más largas, seleccionará menos ejemplos a incluir, mientras que para entradas más cortas seleccionará más.\n",
    "\n",
    "Continuaremos con el ejemplo de la sección anterior, pero esta vez utilizaremos el `LengthBasedExampleSelector` para seleccionar los ejemplos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Give the antonym of every input\n",
      "\n",
      "Word: happy\n",
      "Antonym: sad\n",
      "\n",
      "\n",
      "Word: tall\n",
      "Antonym: short\n",
      "\n",
      "\n",
      "Word: energetic\n",
      "Antonym: lethargic\n",
      "\n",
      "\n",
      "Word: sunny\n",
      "Antonym: gloomy\n",
      "\n",
      "\n",
      "Word: big\n",
      "Antonym:\n"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate, FewShotPromptTemplate\n",
    "from langchain.prompts.example_selector import LengthBasedExampleSelector\n",
    "\n",
    "# Estos son muchos ejemplos de una tarea simulada de creación de antónimos.\n",
    "examples = [\n",
    "    {\"word\": \"happy\", \"antonym\": \"sad\"},\n",
    "    {\"word\": \"tall\", \"antonym\": \"short\"},\n",
    "    {\"word\": \"energetic\", \"antonym\": \"lethargic\"},\n",
    "    {\"word\": \"sunny\", \"antonym\": \"gloomy\"},\n",
    "    {\"word\": \"windy\", \"antonym\": \"calm\"},\n",
    "]\n",
    "\n",
    "# A continuación, especificamos la plantilla para dar formato a los ejemplos que hemos proporcionado.\n",
    "# Para ello utilizamos la clase `PromptTemplate`.\n",
    "example_formatter_template = \"\"\"Word: {word}\n",
    "Antonym: {antonym}\n",
    "\"\"\"\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"word\", \"antonym\"],\n",
    "    template=example_formatter_template,\n",
    ")\n",
    "\n",
    "# Utilizaremos el `LengthBasedExampleSelector` para seleccionar los ejemplos.\n",
    "example_selector = LengthBasedExampleSelector(\n",
    "    # Estos son los ejemplos de los que dispone para elegir.\n",
    "    examples=examples, \n",
    "    # Este es el PromptTemplate que se utiliza para dar formato a los ejemplos.\n",
    "    example_prompt=example_prompt, \n",
    "    # Esta es la longitud máxima que deben tener los ejemplos formateados.\n",
    "    # La longitud se mide con la función get_text_length que aparece a continuación.\n",
    "    max_length=25\n",
    "    # Esta es la función utilizada para obtener la longitud de una cadena, que se utiliza\n",
    "    # para determinar qué ejemplos incluir. Se comenta en la documentación porque\n",
    "    # es opcional. Si no se proporciona, se utiliza la longitud de la cadena.\n",
    "    # get_text_length: Callable[[str], int] = lambda x: len(re.split(\"\\n| \", x))\n",
    ")\n",
    "\n",
    "# Ahora podemos utilizar el `example_selector` para crear un `FewShotPromptTemplate`.\n",
    "dynamic_prompt = FewShotPromptTemplate(\n",
    "    # Proporcionamos un ExampleSelector en lugar de ejemplos.\n",
    "    example_selector=example_selector,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=\"Give the antonym of every input\",\n",
    "    suffix=\"Word: {input}\\nAntonym:\",\n",
    "    input_variables=[\"input\"],\n",
    "    example_separator=\"\\n\\n\",\n",
    ")\n",
    "\n",
    "# Ahora podemos generar un prompt utilizando el método `format`.\n",
    "print(dynamic_prompt.format(input=\"big\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por el contrario, si proporcionamos una entrada muy larga, el `LengthBasedExampleSelector`` seleccionará menos ejemplos para incluir en el prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Give the antonym of every input\n",
      "\n",
      "Word: big and huge and massive and large and gigantic and tall and much much much much much bigger than everything else\n",
      "Antonym:\n"
     ]
    }
   ],
   "source": [
    "long_string = \"big and huge and massive and large and gigantic and tall and much much much much much bigger than everything else\"\n",
    "print(dynamic_prompt.format(input=long_string))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangChain viene con algunos selectores de ejemplo que puedes utilizar. Los verás en el documento ***Prompts - ExampleSelectos - How-To Guides.ipynb***\n",
    "\n",
    "Puedes crear selectores de ejemplo personalizados que seleccionen ejemplos basados en cualquier criterio que desees. Para más detalles sobre cómo hacerlo, consulta ***Prompts - ExampleSelectos - How-To Guides.ipynb***."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat Prompt Templates\n",
    "\n",
    "Los [modelos de chat](https://samusarmiento.hashnode.dev/langchain-101-modelos#heading-chat-models) toman una lista de mensajes de chat como entrada - esta lista se conoce comúnmente como prompt. Estos mensajes de chat difieren de las cadenas de texto sin formato (que pasarías a un modelo LLM) en que cada mensaje está asociado a un rol.\n",
    "\n",
    "Por ejemplo, en OpenAI [Chat Completion API](https://platform.openai.com/docs/guides/gpt/chat-completions-api), un mensaje de chat puede asociarse con el rol de IA, humano o sistema. Se supone que el modelo sigue más de cerca las instrucciones del mensaje de chat del sistema.\n",
    "\n",
    "LangChain proporciona varias plantillas de prompts para facilitar la construcción y el trabajo con prompts. Te animo a utilizar estas plantillas de prompts relacionados con el chat en lugar de PromptTemplate cuando consultes modelos de chat para explotar al máximo el potencial del modelo de chat subyacente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    PromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para crear una plantilla de mensaje asociada a un rol, se utiliza `MessagePromptTemplate`.\n",
    "\n",
    "Por conveniencia, hay un método `from_template` expuesto en la plantilla. Si fueras a utilizar esta plantilla, este es el aspecto que tendría:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "template=\"You are a helpful assistant that translates {input_language} to {output_language}.\"\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
    "human_template=\"{text}\"\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si quisieras construir el `MessagePromptTemplate` más directamente, podrías crear un PromptTemplate fuera y luego pasarlo, ej:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=PromptTemplate(\n",
    "    template=\"You are a helpful assistant that translates {input_language} to {output_language}.\",\n",
    "    input_variables=[\"input_language\", \"output_language\"],\n",
    ")\n",
    "system_message_prompt_2 = SystemMessagePromptTemplate(prompt=prompt)\n",
    "\n",
    "assert system_message_prompt == system_message_prompt_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Después, puedes construir un `ChatPromptTemplate` a partir de uno o más `MessagePromptTemplates`. Puedes usar el `format_prompt` de `ChatPromptTemplate` - esto devuelve un `PromptValue`, que puedes convertir en una string o en un objeto *Message*, dependiendo de si quieres usar el valor formateado como entrada a un modelo llm o chat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a helpful assistant that translates English to French.', additional_kwargs={}),\n",
       " HumanMessage(content='I love programming.', additional_kwargs={}, example=False)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
    "\n",
    "# obtener una finalización de chat a partir de los mensajes formateados\n",
    "chat_prompt.format_prompt(input_language=\"English\", output_language=\"French\", text=\"I love programming.\").to_messages()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formato de salida\n",
    "\n",
    "La salida del método format está disponible como string, lista de mensajes y `ChatPromptValue`\n",
    "\n",
    "como string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'System: You are a helpful assistant that translates English to Spanish.\\nHuman: I love programming.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = chat_prompt.format(input_language=\"English\", output_language=\"Spanish\", text=\"I love programming.\")\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# o en su defecto\n",
    "output_2 = chat_prompt.format_prompt(input_language=\"English\", output_language=\"Spanish\", text=\"I love programming.\").to_string()\n",
    "assert output == output_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como `ChatPromptValue`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are a helpful assistant that translates English to Spanish.', additional_kwargs={}), HumanMessage(content='I love programming.', additional_kwargs={}, example=False)])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_prompt.format_prompt(input_language=\"English\", output_language=\"Spanish\", text=\"I love programming.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como lista de mensajes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a helpful assistant that translates English to Spanish.', additional_kwargs={}),\n",
       " HumanMessage(content='I love programming.', additional_kwargs={}, example=False)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_prompt.format_prompt(input_language=\"English\", output_language=\"Spanish\", text=\"I love programming.\").to_messages()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diferentes tipos de MessagePromptTemplate\n",
    "\n",
    "LangChain proporciona diferentes tipos de `MessagePromptTemplate`. Los más utilizados son `AIMessagePromptTemplate`, `SystemMessagePromptTemplate` y `HumanMessagePromptTemplate`, que crean un mensaje AI, un mensaje de sistema y un mensaje humano respectivamente.\n",
    "\n",
    "Sin embargo, en los casos en los que el modelo de chat admite mensajes de chat con un rol arbitrario, puedes utilizar `ChatMessagePromptTemplate`, que permite al usuario especificar el nombre del rol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatMessage(content='May the force be with you', additional_kwargs={}, role='Jedi')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import ChatMessagePromptTemplate\n",
    "\n",
    "prompt = \"May the {subject} be with you\"\n",
    "\n",
    "chat_message_prompt = ChatMessagePromptTemplate.from_template(role=\"Jedi\", template=prompt)\n",
    "chat_message_prompt.format(subject=\"force\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangChain también proporciona `MessagesPlaceholder`, que le da el control total de los mensajes que se mostrarán durante el formateo. Esto puede ser útil cuando no estás seguro de qué papel debes utilizar para tus plantillas de mensajes o cuando deseas insertar una lista de mensajes durante el formateo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import MessagesPlaceholder\n",
    "\n",
    "human_prompt = \"Summarize our conversation so far in {word_count} words.\"\n",
    "human_message_template = HumanMessagePromptTemplate.from_template(human_prompt)\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([MessagesPlaceholder(variable_name=\"conversation\"), human_message_template])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='What is the best way to learn programming?', additional_kwargs={}, example=False),\n",
       " AIMessage(content='1. Choose a programming language: Decide on a programming language that you want to learn. \\n\\n2. Start with the basics: Familiarize yourself with the basic programming concepts such as variables, data types and control structures.\\n\\n3. Practice, practice, practice: The best way to learn programming is through hands-on experience', additional_kwargs={}, example=False),\n",
       " HumanMessage(content='Summarize our conversation so far in 10 words.', additional_kwargs={}, example=False)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "human_message = HumanMessage(content=\"What is the best way to learn programming?\")\n",
    "\n",
    "chat_prompt.format_prompt(conversation=[human_message, ai_message], word_count=\"10\").to_messages()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Langchain-Tutotials",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
