{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How-To Guides\n",
    "Si es la primera vez que utilizas la biblioteca, te recomendamos que empieces por **Langchain 101: Prompts**.\n",
    "\n",
    "La guía del usuario muestra flujos de trabajo más avanzados y cómo utilizar la biblioteca de diferentes maneras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cómo crear una plantilla de prompts personalizada\n",
    "\n",
    "Digamos que queremos que el LLM genere explicaciones en inglés de una función dado su nombre. Para lograr esta tarea, crearemos una plantilla personalizada que tome el nombre de la función como entrada y formatee la plantilla para proporcionar el código fuente de la función."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¿Por qué se necesitan plantillas de prompts personalizadas?\n",
    "\n",
    "LangChain proporciona un conjunto de plantillas de prompts predeterminadas que se pueden utilizar para generar avisos para una variedad de tareas. Sin embargo, puede haber casos en los que las plantillas predeterminadas no satisfagan nuestras necesidades. Por ejemplo, es posible que queramos crear una plantilla de prompts con instrucciones dinámicas específicas para su modelo lingüístico. En estos casos, podemos crear una plantilla personalizada.\n",
    "\n",
    "Eche un vistazo al conjunto actual de plantillas por defecto [aquí](https://github.com/SamuSarmiento/CursoLangchain/blob/main/prompts/Langchain%20101%20-%20Prompts.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creación de una plantilla personalizada\n",
    "\n",
    "Existen esencialmente dos plantillas distintas de prompts disponibles - `string PromptTemplates` y `chat PromptTemplate`. Las primeras proporcionan un aviso simple en formato string, mientras que las plantillas de prompts de chat producen un prompt más estructurado para ser utilizado con una API de chat.\n",
    "\n",
    "En esta guía, crearemos un prompt personalizado utilizando una plantilla de prompt de string.\n",
    "\n",
    "Para crear una plantilla personalizada, hay dos requisitos:\n",
    "\n",
    "1. Un atributo `input_variables` que expone las variables de entrada que la plantilla espera.\n",
    "\n",
    "2. Exponer un `format method` que toma argumentos de palabras clave correspondientes a las `input_variables` esperadas y devuelve el prompt formateado.\n",
    "\n",
    "Crearemos una plantilla personalizada que tome el nombre de la función como entrada y formatee el prompt para proporcionar el código fuente de la función. Para lograr esto, primero vamos a crear una función que devolverá el código fuente de una función dado su nombre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "\n",
    "def get_source_code(function_name):\n",
    "    # Obtener el código fuente de la función\n",
    "    return inspect.getsource(function_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, vamos a crear una plantilla personalizada que toma el nombre de la función como entrada, y formatea la plantilla para proporcionar el código fuente de la función."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import StringPromptTemplate\n",
    "from pydantic import BaseModel, validator\n",
    "\n",
    "\n",
    "class FunctionExplainerPromptTemplate(StringPromptTemplate, BaseModel):\n",
    "    \"\"\" Una plantilla personalizada que toma el nombre de la función como entrada, y formatea la plantilla para proporcionar el código fuente de la función. \"\"\"\n",
    "\n",
    "    @validator(\"input_variables\")\n",
    "    def validate_input_variables(cls, v):\n",
    "        \"\"\" Validar que las variables de entrada son correctas. \"\"\"\n",
    "        if len(v) != 1 or \"function_name\" not in v:\n",
    "            raise ValueError(\"function_name must be the only input_variable.\")\n",
    "        return v\n",
    "\n",
    "    def format(self, **kwargs) -> str:\n",
    "        # Obtener el código fuente de la función\n",
    "        source_code = get_source_code(kwargs[\"function_name\"])\n",
    "\n",
    "        # Generar el prompt que se enviará al modelo lingüístico\n",
    "        prompt = f\"\"\"\n",
    "        Given the function name and source code, generate an English language explanation of the function.\n",
    "        Function Name: {kwargs[\"function_name\"].__name__}\n",
    "        Source Code:\n",
    "        {source_code}\n",
    "        Explanation:\n",
    "        \"\"\"\n",
    "        return prompt\n",
    "    \n",
    "    def _prompt_type(self):\n",
    "        return \"function-explainer\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilizar la plantilla de prompts personalizada\n",
    "\n",
    "Ahora que hemos creado una plantilla de prompt personalizada, podemos utilizarla para generar avisos para nuestra tarea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        Given the function name and source code, generate an English language explanation of the function.\n",
      "        Function Name: get_source_code\n",
      "        Source Code:\n",
      "        def get_source_code(function_name):\n",
      "    # Obtener el código fuente de la función\n",
      "    return inspect.getsource(function_name)\n",
      "\n",
      "        Explanation:\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "fn_explainer = FunctionExplainerPromptTemplate(input_variables=[\"function_name\"])\n",
    "\n",
    "# Generar un prompt para la función \"get_source_code\"\n",
    "prompt = fn_explainer.format(function_name=get_source_code)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cómo crear una plantilla de prompts que use few shot examples\n",
    "\n",
    "En este tutorial, aprenderemos a crear una plantilla de prompt que utilice few shot examples.\n",
    "\n",
    "Utilizaremos la clase `FewShotPromptTemplate` para crear la plantilla. Esta clase recibe un conjunto de ejemplos o un objeto `ExampleSelector`. En este tutorial, veremos ambas opciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caso de uso\n",
    "\n",
    "En este tutorial, vamos a configurar algunos ejemplos para la búsqueda online de las auto-preguntas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilización de un conjunto de ejemplos\n",
    "\n",
    "### Crear el conjunto de ejemplos\n",
    "\n",
    "Para empezar, cree una lista de ejemplos. Cada ejemplo debe ser un diccionario con las claves siendo las variables de entrada y los valores siendo los valores para esas variables de entrada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "examples = [\n",
    "  {\n",
    "    \"question\": \"Who lived longer, Muhammad Ali or Alan Turing?\",\n",
    "    \"answer\": \n",
    "\"\"\"\n",
    "Are follow up questions needed here: Yes.\n",
    "Follow up: How old was Muhammad Ali when he died?\n",
    "Intermediate answer: Muhammad Ali was 74 years old when he died.\n",
    "Follow up: How old was Alan Turing when he died?\n",
    "Intermediate answer: Alan Turing was 41 years old when he died.\n",
    "So the final answer is: Muhammad Ali\n",
    "\"\"\"\n",
    "  },\n",
    "  {\n",
    "    \"question\": \"When was the founder of craigslist born?\",\n",
    "    \"answer\": \n",
    "\"\"\"\n",
    "Are follow up questions needed here: Yes.\n",
    "Follow up: Who was the founder of craigslist?\n",
    "Intermediate answer: Craigslist was founded by Craig Newmark.\n",
    "Follow up: When was Craig Newmark born?\n",
    "Intermediate answer: Craig Newmark was born on December 6, 1952.\n",
    "So the final answer is: December 6, 1952\n",
    "\"\"\"\n",
    "  },\n",
    "  {\n",
    "    \"question\": \"Who was the maternal grandfather of George Washington?\",\n",
    "    \"answer\":\n",
    "\"\"\"\n",
    "Are follow up questions needed here: Yes.\n",
    "Follow up: Who was the mother of George Washington?\n",
    "Intermediate answer: The mother of George Washington was Mary Ball Washington.\n",
    "Follow up: Who was the father of Mary Ball Washington?\n",
    "Intermediate answer: The father of Mary Ball Washington was Joseph Ball.\n",
    "So the final answer is: Joseph Ball\n",
    "\"\"\"\n",
    "  },\n",
    "  {\n",
    "    \"question\": \"Are both the directors of Jaws and Casino Royale from the same country?\",\n",
    "    \"answer\":\n",
    "\"\"\"\n",
    "Are follow up questions needed here: Yes.\n",
    "Follow up: Who is the director of Jaws?\n",
    "Intermediate Answer: The director of Jaws is Steven Spielberg.\n",
    "Follow up: Where is Steven Spielberg from?\n",
    "Intermediate Answer: The United States.\n",
    "Follow up: Who is the director of Casino Royale?\n",
    "Intermediate Answer: The director of Casino Royale is Martin Campbell.\n",
    "Follow up: Where is Martin Campbell from?\n",
    "Intermediate Answer: New Zealand.\n",
    "So the final answer is: No\n",
    "\"\"\"\n",
    "  }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crear un formateador\n",
    "\n",
    "Configura un formateador para los ejemplos en una string. Este formateador debe ser un objeto PromptTemplate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Who lived longer, Muhammad Ali or Alan Turing?\n",
      "\n",
      "Are follow up questions needed here: Yes.\n",
      "Follow up: How old was Muhammad Ali when he died?\n",
      "Intermediate answer: Muhammad Ali was 74 years old when he died.\n",
      "Follow up: How old was Alan Turing when he died?\n",
      "Intermediate answer: Alan Turing was 41 years old when he died.\n",
      "So the final answer is: Muhammad Ali\n",
      "\n"
     ]
    }
   ],
   "source": [
    "example_prompt = PromptTemplate(input_variables=[\"question\", \"answer\"], template=\"Question: {question}\\n{answer}\")\n",
    "\n",
    "print(example_prompt.format(**examples[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alimentar los ejemplos y el formateador a `FewShotPromptTemplate`\n",
    "\n",
    "Por último, crear un objeto `FewShotPromptTemplate`. Este objeto recibe los Few Shot Examples y el formateador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Who lived longer, Muhammad Ali or Alan Turing?\n",
      "\n",
      "Are follow up questions needed here: Yes.\n",
      "Follow up: How old was Muhammad Ali when he died?\n",
      "Intermediate answer: Muhammad Ali was 74 years old when he died.\n",
      "Follow up: How old was Alan Turing when he died?\n",
      "Intermediate answer: Alan Turing was 41 years old when he died.\n",
      "So the final answer is: Muhammad Ali\n",
      "\n",
      "\n",
      "Question: When was the founder of craigslist born?\n",
      "\n",
      "Are follow up questions needed here: Yes.\n",
      "Follow up: Who was the founder of craigslist?\n",
      "Intermediate answer: Craigslist was founded by Craig Newmark.\n",
      "Follow up: When was Craig Newmark born?\n",
      "Intermediate answer: Craig Newmark was born on December 6, 1952.\n",
      "So the final answer is: December 6, 1952\n",
      "\n",
      "\n",
      "Question: Who was the maternal grandfather of George Washington?\n",
      "\n",
      "Are follow up questions needed here: Yes.\n",
      "Follow up: Who was the mother of George Washington?\n",
      "Intermediate answer: The mother of George Washington was Mary Ball Washington.\n",
      "Follow up: Who was the father of Mary Ball Washington?\n",
      "Intermediate answer: The father of Mary Ball Washington was Joseph Ball.\n",
      "So the final answer is: Joseph Ball\n",
      "\n",
      "\n",
      "Question: Are both the directors of Jaws and Casino Royale from the same country?\n",
      "\n",
      "Are follow up questions needed here: Yes.\n",
      "Follow up: Who is the director of Jaws?\n",
      "Intermediate Answer: The director of Jaws is Steven Spielberg.\n",
      "Follow up: Where is Steven Spielberg from?\n",
      "Intermediate Answer: The United States.\n",
      "Follow up: Who is the director of Casino Royale?\n",
      "Intermediate Answer: The director of Casino Royale is Martin Campbell.\n",
      "Follow up: Where is Martin Campbell from?\n",
      "Intermediate Answer: New Zealand.\n",
      "So the final answer is: No\n",
      "\n",
      "\n",
      "Question: Who was the father of Mary Ball Washington?\n"
     ]
    }
   ],
   "source": [
    "prompt = FewShotPromptTemplate(\n",
    "    examples=examples, \n",
    "    example_prompt=example_prompt, \n",
    "    suffix=\"Question: {input}\", \n",
    "    input_variables=[\"input\"]\n",
    ")\n",
    "\n",
    "print(prompt.format(input=\"Who was the father of Mary Ball Washington?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilizar un selector de ejemplos\n",
    "\n",
    "### Introducir ejemplos en `ExampleSelector`\n",
    "\n",
    "Reutilizaremos el conjunto de ejemplos y el formateador de la sección anterior. Sin embargo, en lugar de introducir los ejemplos directamente en el objeto `FewShotPromptTemplate`, los introduciremos en un objeto `ExampleSelector`.\n",
    "\n",
    "En este tutorial, utilizaremos la clase `SemanticSimilarityExampleSelector`. Esta clase selecciona algunos ejemplos de disparos basándose en su similitud con la entrada. Utiliza un modelo de incrustación para calcular la similitud entre la entrada y los pocos ejemplos de disparo, así como un almacén de vectores para realizar la búsqueda del vecino más cercano."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: chromadb in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (0.3.26)\n",
      "Requirement already satisfied: uvicorn[standard]>=0.18.3 in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (from chromadb) (0.22.0)\n",
      "Requirement already satisfied: requests>=2.28 in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (from chromadb) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (from chromadb) (4.65.0)\n",
      "Requirement already satisfied: hnswlib>=0.7 in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (from chromadb) (0.7.0)\n",
      "Requirement already satisfied: pandas>=1.3 in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (from chromadb) (2.0.2)\n",
      "Requirement already satisfied: numpy>=1.21.6 in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (from chromadb) (1.24.3)\n",
      "Requirement already satisfied: posthog>=2.4.0 in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (from chromadb) (3.0.1)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (from chromadb) (1.15.0)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (from chromadb) (0.13.3)\n",
      "Requirement already satisfied: pydantic>=1.9 in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (from chromadb) (1.10.8)\n",
      "Requirement already satisfied: duckdb>=0.7.1 in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (from chromadb) (0.8.1)\n",
      "Requirement already satisfied: clickhouse-connect>=0.5.7 in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (from chromadb) (0.6.2)\n",
      "Requirement already satisfied: fastapi>=0.85.1 in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (from chromadb) (0.97.0)\n",
      "Requirement already satisfied: pulsar-client>=3.1.0 in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (from chromadb) (3.2.0)\n",
      "Requirement already satisfied: overrides>=7.3.1 in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (from chromadb) (7.3.1)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (from chromadb) (4.6.2)\n",
      "Requirement already satisfied: zstandard in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (from clickhouse-connect>=0.5.7->chromadb) (0.21.0)\n",
      "Requirement already satisfied: pytz in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (from clickhouse-connect>=0.5.7->chromadb) (2023.3)\n",
      "Requirement already satisfied: certifi in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (from clickhouse-connect>=0.5.7->chromadb) (2023.5.7)\n",
      "Requirement already satisfied: urllib3>=1.26 in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (from clickhouse-connect>=0.5.7->chromadb) (2.0.2)\n",
      "Requirement already satisfied: lz4 in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (from clickhouse-connect>=0.5.7->chromadb) (4.3.2)\n",
      "Requirement already satisfied: starlette<0.28.0,>=0.27.0 in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (from fastapi>=0.85.1->chromadb) (0.27.0)\n",
      "Requirement already satisfied: protobuf in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (4.23.2)\n",
      "Requirement already satisfied: coloredlogs in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (1.12)\n",
      "Requirement already satisfied: flatbuffers in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (23.5.26)\n",
      "Requirement already satisfied: packaging in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (23.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (from pandas>=1.3->chromadb) (2023.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (from pandas>=1.3->chromadb) (2.8.2)\n",
      "Requirement already satisfied: monotonic>=1.5 in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (from posthog>=2.4.0->chromadb) (1.6)\n",
      "Requirement already satisfied: backoff>=1.10.0 in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (from posthog>=2.4.0->chromadb) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (from requests>=2.28->chromadb) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (from requests>=2.28->chromadb) (3.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (from tqdm>=4.65.0->chromadb) (0.4.6)\n",
      "Requirement already satisfied: h11>=0.8 in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.14.0)\n",
      "Requirement already satisfied: click>=7.0 in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (8.1.3)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.0)\n",
      "Requirement already satisfied: httptools>=0.5.0 in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.5.0)\n",
      "Requirement already satisfied: websockets>=10.4 in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (11.0.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (6.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.19.0)\n",
      "Requirement already satisfied: anyio<5,>=3.4.0 in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (from starlette<0.28.0,>=0.27.0->fastapi>=0.85.1->chromadb) (3.7.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi>=0.85.1->chromadb) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi>=0.85.1->chromadb) (1.1.1)\n",
      "Requirement already satisfied: pyreadline3 in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (from humanfriendly>=9.1->coloredlogs->onnxruntime>=1.14.1->chromadb) (3.4.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.example_selector import SemanticSimilarityExampleSelector\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples most similar to the input: Who was the father of Mary Ball Washington?\n",
      "\n",
      "\n",
      "question: Who was the maternal grandfather of George Washington?\n",
      "answer: \n",
      "Are follow up questions needed here: Yes.\n",
      "Follow up: Who was the mother of George Washington?\n",
      "Intermediate answer: The mother of George Washington was Mary Ball Washington.\n",
      "Follow up: Who was the father of Mary Ball Washington?\n",
      "Intermediate answer: The father of Mary Ball Washington was Joseph Ball.\n",
      "So the final answer is: Joseph Ball\n",
      "\n"
     ]
    }
   ],
   "source": [
    "example_selector = SemanticSimilarityExampleSelector.from_examples(\n",
    "    # Esta es la lista de ejemplos disponibles para seleccionar.\n",
    "    examples,\n",
    "    # Esta es la clase de embedding utilizada para producir incrustaciones que se utilizan para medir la similitud semántica.\n",
    "    OpenAIEmbeddings(),\n",
    "    # Esta es la clase VectorStore que se utiliza para almacenar las incrustaciones y hacer una búsqueda de similitud sobre.\n",
    "    Chroma,\n",
    "    # Este es el número de ejemplos a producir.\n",
    "    k=1\n",
    ")\n",
    "\n",
    "# Selecciona el ejemplo más similar a la entrada.\n",
    "question = \"Who was the father of Mary Ball Washington?\"\n",
    "selected_examples = example_selector.select_examples({\"question\": question})\n",
    "print(f\"Examples most similar to the input: {question}\")\n",
    "for example in selected_examples:\n",
    "    print(\"\\n\")\n",
    "    for k, v in example.items():\n",
    "        print(f\"{k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introducir el selector de ejemplo en `FewShotPromptTemplate`\n",
    "\n",
    "Por último, crea un objeto `FewShotPromptTemplate`. Este objeto toma el selector de ejemplo y el formateador para los ejemplos de pocas tomas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Who was the maternal grandfather of George Washington?\n",
      "\n",
      "Are follow up questions needed here: Yes.\n",
      "Follow up: Who was the mother of George Washington?\n",
      "Intermediate answer: The mother of George Washington was Mary Ball Washington.\n",
      "Follow up: Who was the father of Mary Ball Washington?\n",
      "Intermediate answer: The father of Mary Ball Washington was Joseph Ball.\n",
      "So the final answer is: Joseph Ball\n",
      "\n",
      "\n",
      "Question: Who was the father of Mary Ball Washington?\n"
     ]
    }
   ],
   "source": [
    "prompt = FewShotPromptTemplate(\n",
    "    example_selector=example_selector, \n",
    "    example_prompt=example_prompt, \n",
    "    suffix=\"Question: {input}\", \n",
    "    input_variables=[\"input\"]\n",
    ")\n",
    "\n",
    "print(prompt.format(input=\"Who was the father of Mary Ball Washington?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Composición de prompts\n",
    "\n",
    "Este cuaderno explica cómo componer varias instrucciones juntas. Esto puede ser útil cuando se quiere reutilizar partes de prompts. Esto se puede hacer con un PipelinePrompt. Un PipelinePrompt consta de dos partes principales:\n",
    "\n",
    "- final_prompt: Este es el prompt final que se devuelve\n",
    "\n",
    "- pipeline_prompts: Es una lista de tuplas, formada por una string (`name`) y un Prompt Template. Cada PromptTemplate será formateado y luego pasado a futuras plantillas de prompt como una variable con el mismo nombre que `name`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.pipeline import PipelinePromptTemplate\n",
    "from langchain.prompts.prompt import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_template = \"\"\"{introduction}\n",
    "\n",
    "{example}\n",
    "\n",
    "{start}\"\"\"\n",
    "full_prompt = PromptTemplate.from_template(full_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "introduction_template = \"\"\"You are impersonating {person}.\"\"\"\n",
    "introduction_prompt = PromptTemplate.from_template(introduction_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_template = \"\"\"Here's an example of an interaction: \n",
    "\n",
    "Q: {example_q}\n",
    "A: {example_a}\"\"\"\n",
    "example_prompt = PromptTemplate.from_template(example_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_prompts = [\n",
    "    (\"introduction\", introduction_prompt),\n",
    "    (\"example\", example_prompt),\n",
    "    (\"start\", start_prompt)\n",
    "]\n",
    "pipeline_prompt = PipelinePromptTemplate(final_prompt=full_prompt, pipeline_prompts=input_prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_prompt.input_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pipeline_prompt.format(\n",
    "    person=\"Elon Musk\",\n",
    "    example_q=\"What's your favorite car?\",\n",
    "    example_a=\"Telsa\",\n",
    "    input=\"What's your favorite social media site?\"\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cómo serializar prompts\n",
    "\n",
    "A menudo es preferible almacenar los prompts no como código python sino como archivos. Esto puede facilitar compartir, almacenar y versionar prompts. Este cuaderno cubre cómo hacerlo en LangChain, recorriendo los diferentes tipos de prompts y las diferentes opciones de serialización.\n",
    "\n",
    "En un alto nivel, los siguientes principios de diseño se aplican a la serialización:\n",
    "\n",
    "1. Tanto JSON y YAML son compatibles. Soporta métodos de serialización que sean legibles por humanos en disco, y YAML y JSON son dos de los métodos más populares para ello. Ten en cuenta que esta regla se aplica a los prompts. Para otros activos, se pueden admitir diferentes métodos de serialización.\n",
    "\n",
    "2. Apoyan especificar todo en un archivo, o almacenar diferentes componentes (plantillas, ejemplos, etc) en diferentes archivos y referenciarlos. Para algunos casos, almacenar todo en un archivo tiene más sentido, pero para otros es preferible dividir algunos de los activos (plantillas largas, ejemplos grandes, componentes reutilizables). LangChain admite ambas opciones.\n",
    "\n",
    "También hay un único punto de entrada para cargar avisos desde el disco, lo que facilita la carga de cualquier tipo de aviso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Todos los prompts se cargan a través de la función `load_prompt`.\n",
    "from langchain.prompts import load_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PromptTemplate\n",
    "\n",
    "Esta sección cubre ejemplos para cargar un PromptTemplate.\n",
    "\n",
    "#### Carga desde YAML\n",
    "\n",
    "Esto muestra un ejemplo de carga de un PromptTemplate desde YAML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting cat-python\n",
      "  Downloading cat_python-1.0-py3-none-any.whl (10 kB)\n",
      "Collecting XlsxWriter\n",
      "  Downloading XlsxWriter-3.1.2-py3-none-any.whl (153 kB)\n",
      "     -------------------------------------- 153.0/153.0 kB 4.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (from cat-python) (1.24.3)\n",
      "Collecting statsmodels\n",
      "  Downloading statsmodels-0.14.0-cp310-cp310-win_amd64.whl (9.2 MB)\n",
      "     ---------------------------------------- 9.2/9.2 MB 20.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: tqdm in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (from cat-python) (4.65.0)\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.7.1-cp310-cp310-win_amd64.whl (7.6 MB)\n",
      "     ---------------------------------------- 7.6/7.6 MB 28.6 MB/s eta 0:00:00\n",
      "Collecting xlrd\n",
      "  Downloading xlrd-2.0.1-py2.py3-none-any.whl (96 kB)\n",
      "     ---------------------------------------- 96.5/96.5 kB ? eta 0:00:00\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.2.2-cp310-cp310-win_amd64.whl (8.3 MB)\n",
      "     ---------------------------------------- 8.3/8.3 MB 33.2 MB/s eta 0:00:00\n",
      "Collecting scanpy>=1.5.1\n",
      "  Downloading scanpy-1.9.3-py3-none-any.whl (2.0 MB)\n",
      "     ---------------------------------------- 2.0/2.0 MB 42.9 MB/s eta 0:00:00\n",
      "Collecting anndata\n",
      "  Downloading anndata-0.9.1-py3-none-any.whl (102 kB)\n",
      "     -------------------------------------- 103.0/103.0 kB 5.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: Jinja2 in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (from cat-python) (3.1.2)\n",
      "Requirement already satisfied: pandas in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (from cat-python) (2.0.2)\n",
      "Collecting scipy\n",
      "  Downloading scipy-1.10.1-cp310-cp310-win_amd64.whl (42.5 MB)\n",
      "     --------------------------------------- 42.5/42.5 MB 40.9 MB/s eta 0:00:00\n",
      "Collecting h5py>=3\n",
      "  Downloading h5py-3.8.0-cp310-cp310-win_amd64.whl (2.6 MB)\n",
      "     ---------------------------------------- 2.6/2.6 MB 41.6 MB/s eta 0:00:00\n",
      "Collecting numba>=0.41.0\n",
      "  Downloading numba-0.57.0-cp310-cp310-win_amd64.whl (2.6 MB)\n",
      "     ---------------------------------------- 2.6/2.6 MB 54.1 MB/s eta 0:00:00\n",
      "Collecting networkx>=2.3\n",
      "  Using cached networkx-3.1-py3-none-any.whl (2.1 MB)\n",
      "Collecting seaborn\n",
      "  Downloading seaborn-0.12.2-py3-none-any.whl (293 kB)\n",
      "     ------------------------------------- 293.3/293.3 kB 18.9 MB/s eta 0:00:00\n",
      "Collecting patsy\n",
      "  Downloading patsy-0.5.3-py2.py3-none-any.whl (233 kB)\n",
      "     ------------------------------------- 233.8/233.8 kB 14.9 MB/s eta 0:00:00\n",
      "Collecting joblib\n",
      "  Using cached joblib-1.2.0-py3-none-any.whl (297 kB)\n",
      "Collecting umap-learn>=0.3.10\n",
      "  Downloading umap-learn-0.5.3.tar.gz (88 kB)\n",
      "     ---------------------------------------- 88.2/88.2 kB 5.2 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: packaging in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (from scanpy>=1.5.1->cat-python) (23.1)\n",
      "Collecting session-info\n",
      "  Downloading session_info-1.0.0.tar.gz (24 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting natsort\n",
      "  Downloading natsort-8.3.1-py3-none-any.whl (38 kB)\n",
      "Collecting pyparsing>=2.3.1\n",
      "  Using cached pyparsing-3.0.9-py3-none-any.whl (98 kB)\n",
      "Collecting kiwisolver>=1.0.1\n",
      "  Downloading kiwisolver-1.4.4-cp310-cp310-win_amd64.whl (55 kB)\n",
      "     ---------------------------------------- 55.3/55.3 kB 3.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (from matplotlib->cat-python) (2.8.2)\n",
      "Collecting fonttools>=4.22.0\n",
      "  Downloading fonttools-4.40.0-cp310-cp310-win_amd64.whl (1.9 MB)\n",
      "     ---------------------------------------- 1.9/1.9 MB 62.5 MB/s eta 0:00:00\n",
      "Collecting contourpy>=1.0.1\n",
      "  Downloading contourpy-1.1.0-cp310-cp310-win_amd64.whl (470 kB)\n",
      "     ------------------------------------- 470.4/470.4 kB 28.8 MB/s eta 0:00:00\n",
      "Collecting cycler>=0.10\n",
      "  Downloading cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
      "Collecting pillow>=6.2.0\n",
      "  Using cached Pillow-9.5.0-cp310-cp310-win_amd64.whl (2.5 MB)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (from pandas->cat-python) (2023.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (from pandas->cat-python) (2023.3)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Using cached threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (from Jinja2->cat-python) (2.1.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (from tqdm->cat-python) (0.4.6)\n",
      "Collecting llvmlite<0.41,>=0.40.0dev0\n",
      "  Downloading llvmlite-0.40.1rc1-cp310-cp310-win_amd64.whl (27.7 MB)\n",
      "     --------------------------------------- 27.7/27.7 MB 46.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: six in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (from patsy->scanpy>=1.5.1->cat-python) (1.16.0)\n",
      "Collecting pynndescent>=0.5\n",
      "  Downloading pynndescent-0.5.10.tar.gz (1.1 MB)\n",
      "     ---------------------------------------- 1.1/1.1 MB 75.2 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting stdlib_list\n",
      "  Downloading stdlib_list-0.8.0-py3-none-any.whl (63 kB)\n",
      "     ---------------------------------------- 63.5/63.5 kB ? eta 0:00:00\n",
      "Building wheels for collected packages: umap-learn, session-info, pynndescent\n",
      "  Building wheel for umap-learn (setup.py): started\n",
      "  Building wheel for umap-learn (setup.py): finished with status 'done'\n",
      "  Created wheel for umap-learn: filename=umap_learn-0.5.3-py3-none-any.whl size=82909 sha256=8a1ea45b3c975efd1cb788d47b5bae04363257cede24c739018641a8cfe18fe6\n",
      "  Stored in directory: c:\\users\\user\\appdata\\local\\pip\\cache\\wheels\\a0\\e8\\c6\\a37ea663620bd5200ea1ba0907ab3c217042c1d035ef606acc\n",
      "  Building wheel for session-info (setup.py): started\n",
      "  Building wheel for session-info (setup.py): finished with status 'done'\n",
      "  Created wheel for session-info: filename=session_info-1.0.0-py3-none-any.whl size=8073 sha256=6a720a96c71a7060f4ed9609f27d99f170c88bc8f7ee73ea377e49c5837d7b1b\n",
      "  Stored in directory: c:\\users\\user\\appdata\\local\\pip\\cache\\wheels\\6a\\aa\\b9\\eb5d4031476ec10802795b97ccf937b9bd998d68a9b268765a\n",
      "  Building wheel for pynndescent (setup.py): started\n",
      "  Building wheel for pynndescent (setup.py): finished with status 'done'\n",
      "  Created wheel for pynndescent: filename=pynndescent-0.5.10-py3-none-any.whl size=55676 sha256=ba317f094a7273e5db85ba1cdcfb5a36d3e435778df985f17734ea5c0f14679e\n",
      "  Stored in directory: c:\\users\\user\\appdata\\local\\pip\\cache\\wheels\\4a\\38\\5d\\f60a40a66a9512b7e5e83517ebc2d1b42d857be97d135f1096\n",
      "Successfully built umap-learn session-info pynndescent\n",
      "Installing collected packages: stdlib_list, XlsxWriter, xlrd, threadpoolctl, session-info, scipy, pyparsing, pillow, patsy, networkx, natsort, llvmlite, kiwisolver, joblib, h5py, fonttools, cycler, contourpy, scikit-learn, numba, matplotlib, statsmodels, seaborn, pynndescent, anndata, umap-learn, scanpy, cat-python\n",
      "Successfully installed XlsxWriter-3.1.2 anndata-0.9.1 cat-python-1.0 contourpy-1.1.0 cycler-0.11.0 fonttools-4.40.0 h5py-3.8.0 joblib-1.2.0 kiwisolver-1.4.4 llvmlite-0.40.1rc1 matplotlib-3.7.1 natsort-8.3.1 networkx-3.1 numba-0.57.0 patsy-0.5.3 pillow-9.5.0 pynndescent-0.5.10 pyparsing-3.0.9 scanpy-1.9.3 scikit-learn-1.2.2 scipy-1.10.1 seaborn-0.12.2 session-info-1.0.0 statsmodels-0.14.0 stdlib_list-0.8.0 threadpoolctl-3.1.0 umap-learn-0.5.3 xlrd-2.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install cat-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat simple_prompt.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = load_prompt(\"simple_prompt.yaml\")\n",
    "print(prompt.format(adjective=\"funny\", content=\"chickens\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carga desde JSON\n",
    "\n",
    "Esto muestra un ejemplo de carga de un PromptTemplate desde JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat simple_prompt.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = load_prompt(\"simple_prompt.json\")\n",
    "print(prompt.format(adjective=\"funny\", content=\"chickens\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargar plantilla desde un archivo\n",
    "\n",
    "Esto muestra un ejemplo de almacenar la plantilla en un archivo separado y luego referenciarlo en el config. Observa que la clave cambia de `template` a `template_path`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat simple_template.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat simple_prompt_with_template_file.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = load_prompt(\"simple_prompt_with_template_file.json\")\n",
    "print(prompt.format(adjective=\"funny\", content=\"chickens\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FewShotPromptTemplate\n",
    "\n",
    "Esta sección cubre ejemplos para cargar few shot prompt templates.\n",
    "\n",
    "## Ejemplos\n",
    "Esto muestra un ejemplo de cómo se verían los ejemplos almacenados como json."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat few_shot_prompt.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = load_prompt(\"few_shot_prompt.json\")\n",
    "print(prompt.format(adjective=\"funny\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat few_shot_prompt.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = load_prompt(\"few_shot_prompt.yaml\")\n",
    "print(prompt.format(adjective=\"funny\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat few_shot_prompt_yaml_examples.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = load_prompt(\"few_shot_prompt_yaml_examples.yaml\")\n",
    "print(prompt.format(adjective=\"funny\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PromptTempalte con OutputParser\n",
    "\n",
    "Esto muestra un ejemplo de cargar un prompt junto con un OutputParser desde un fichero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat prompt_with_output_parser.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = load_prompt(\"prompt_with_output_parser.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt.output_parser.parse(\"George Washington was born in 1732 and died in 1799.\\nScore: 1/2\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Langchain-Tutotials",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
