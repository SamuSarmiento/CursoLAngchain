{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Langchain 101: Modelos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este tutorial te brindaré una visión general sobre los diferentes tipos de modelos que se utilizan en Langchain"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducción\n",
    "\n",
    "Esta sección de la documentación trata de los diferentes tipos de modelos que se utilizan en LangChain. En esta página repasaremos los tipos de modelos en un nivel alto, pero tendremos artículos e hilos en Twitter individuales para cada tipo de modelo.\n",
    "\n",
    "Uno de los principales valores de LangChain es que proporciona una interfaz estándar para los modelos. Esto permite intercambiar fácilmente entre modelos. A alto nivel, hay dos tipos principales de modelos:\n",
    "\n",
    "- **Modelos lingüísticos**: buenos para la generación de texto\n",
    "\n",
    "- **Modelos de incrustación de texto**: buenos para convertir texto en una representación numérica."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos lingüísticos\n",
    "\n",
    "Existen dos subtipos diferentes de modelos lingüísticos:\n",
    "\n",
    "- **LLMs**: envuelven APIs que reciben y devuelven texto.\n",
    "\n",
    "- **ChatModels**: modelos que reciben mensajes de chat y devuelven un mensaje de chat.\n",
    "\n",
    "Se trata de una diferencia sutil, pero un valor añadido de LangChain es que proporciona una interfaz unificada para todos ellos. Esto es bueno porque, aunque las API subyacentes son en realidad muy diferentes, a menudo se desea utilizarlas indistintamente.\n",
    "\n",
    "Para ver esto, echemos un vistazo a OpenAI (un wrapper del LLM de OpenAI) vs ChatOpenAI (un wrapper alrededor del ChatModel de OpenAI)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_model = ChatOpenAI()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interfaz *Text to Text*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.predict(\"say hi!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_model.predict(\"say hi!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interfaz *Messages to Messages*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.predict_messages([HumanMessage(content=\"say hi!\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_model.predict_messages([HumanMessage(content=\"say hi!\")])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLMS\n",
    "\n",
    "Los grandes modelos lingüísticos (LLM) son un componente central de LangChain. LangChain no es un proveedor de LLMs, sino que proporciona una interfaz estándar a través de la cual se puede interactuar con una variedad de LLMs."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cómo empezar\n",
    "\n",
    "En este artículo y en su respectivo cuaderno explicamos cómo utilizar la clase LLM en LangChain.\n",
    "\n",
    "La clase LLM es una clase diseñada para interactuar con LLMs. Hay un montón de proveedores LLM (OpenAI, Cohere, Hugging Face, etc) - esta clase está diseñada para proporcionar una interfaz estándar para todos ellos. En esta parte de la documentación, nos centraremos en la funcionalidad genérica de los LLM.\n",
    "\n",
    "Para este cuaderno, trabajaremos con un wrapper LLM de OpenAI, aunque las funcionalidades destacadas son genéricas para todos los tipos de LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(model_name=\"text-ada-001\", n=2, best_of=2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generar Texto**: La funcionalidad más básica de un LLM es sólo la capacidad de llamarlo, pasando una cadena y obteniendo de vuelta una cadena."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm(\"Tell me a joke\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generar**: En términos más generales, puedes llamarlo con una lista de inputs, obteniendo de vuelta una respuesta más completa que sólo el texto. Esta respuesta completa incluye cosas como múltiples respuestas principales, así como información específica del proveedor de LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_result = llm.generate([\"Tell me a joke\", \"Tell me a poem\"]*15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(llm_result.generations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_result.generations[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_result.generations[-1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También puede acceder a la información específica del proveedor que se devuelve. Esta información NO está estandarizada entre proveedores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_result.llm_output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Número de tokens**: También puedes estimar cuántos tokens tendrá un trozo de texto en ese modelo. Esto es útil porque los modelos tienen una longitud de contexto (y cuestan más por más tokens), lo que significa que necesitas ser consciente de lo largo que es el texto que estás pasando.\n",
    "\n",
    "Ten en cuenta que, por defecto, los tokens se estiman utilizando tiktoken (excepto para la versión heredada <3.8, donde se utiliza un tokenizador Hugging Face)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.get_num_tokens(\"what a joke\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funcionalidad general\n",
    "\n",
    "Todos los ejemplos aquí expuestos abordan ciertas guías \"prácticas\" para trabajar con LLM.\n",
    "\n",
    ">Todas las guías aquí mencionadas son en inglés, de la documentación oficial de Langchain.\n",
    ">\n",
    ">>Si veo apoyo en el contenido y ⭐ el repo de Github iré trayendo \"How To's\" en castellano\n",
    "\n",
    "- [How to use the async API for LLMs](https://python.langchain.com/en/latest/modules/models/llms/examples/async_llm.html)\n",
    "- [How to write a custom LLM wrapper](https://python.langchain.com/en/latest/modules/models/llms/examples/custom_llm.html)\n",
    "- [How (and why) to use the fake LLM](https://python.langchain.com/en/latest/modules/models/llms/examples/fake_llm.html)\n",
    "- [How (and why) to use the human input LLM](https://python.langchain.com/en/latest/modules/models/llms/examples/human_input_llm.html)\n",
    "- [How to cache LLM calls](https://python.langchain.com/en/latest/modules/models/llms/examples/llm_caching.html)\n",
    "- [How to serialize LLM classes](https://python.langchain.com/en/latest/modules/models/llms/examples/llm_serialization.html)\n",
    "- [How to stream LLM and Chat Model responses](https://python.langchain.com/en/latest/modules/models/llms/examples/streaming_llm.html)\n",
    "- [How to track token usage](https://python.langchain.com/en/latest/modules/models/llms/examples/token_usage_tracking.html)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integraciones\n",
    "\n",
    "Los ejemplos que aquí se presentan son guías prácticas sobre cómo integrarse con distintos proveedores de LLM.\n",
    "\n",
    ">Todas las guías aquí mencionadas son en inglés, de la documentación oficial de Langchain.\n",
    "\n",
    "https://python.langchain.com/en/latest/modules/models/llms/integrations.html\n",
    "\n",
    "___"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat Models\n",
    "\n",
    "Los modelos de chat son una variación de los modelos lingüísticos. Aunque los modelos de chat utilizan modelos de lenguaje, la interfaz que exponen es un poco diferente. En lugar de exponer una API de \"entrada de texto, salida de texto\", exponen una interfaz en la que los \"mensajes de chat\" son las entradas y salidas.\n",
    "\n",
    "Las API de los modelos de chat son bastante nuevas, por lo que aún estamos tratando de encontrar las abstracciones correctas."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cómo empezar\n",
    "\n",
    "En este artículo y en su respectivo cuaderno explicamo cómo empezar con los modelos de chat. La interfaz se basa en mensajes y no en texto sin formato."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatOpenAI(temperature=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puedes obtener finalizaciones de chat pasando uno o más mensajes al modelo de chat. La respuesta será un mensaje. Los tipos de mensajes actualmente soportados en LangChain son `AIMessage`, `HumanMessage`, `SystemMessage`, y `ChatMessage` - `ChatMessage` toma un parámetro de rol arbitrario. La mayoría de las veces, sólo tendrás que tratar con `HumanMessage`, `AIMessage` y `SystemMessage`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat([HumanMessage(content=\"Translate this sentence from English to French. I love programming.\")])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El modelo de chat de OpenAI soporta múltiples mensajes como input. Ver [aquí](https://platform.openai.com/docs/guides/gpt/chat-completions-vs-completions) para más información. \n",
    "\n",
    "He aquí un ejemplo de envío de un mensaje de sistema y de usuario al modelo de chat:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant that translates English to French.\"),\n",
    "    HumanMessage(content=\"I love programming.\")\n",
    "]\n",
    "chat(messages)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puedes ir un paso más allá y generar terminaciones para múltiples conjuntos de mensajes utilizando `generate`. Esto devuelve un `LLMResult` con un parámetro de `message` adicional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_messages = [\n",
    "    [\n",
    "        SystemMessage(content=\"You are a helpful assistant that translates English to French.\"),\n",
    "        HumanMessage(content=\"I love programming.\")\n",
    "    ],\n",
    "    [\n",
    "        SystemMessage(content=\"You are a helpful assistant that translates English to French.\"),\n",
    "        HumanMessage(content=\"I love artificial intelligence.\")\n",
    "    ],\n",
    "]\n",
    "result = chat.generate(batch_messages)\n",
    "result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PromptTemplates\n",
    "\n",
    "Puedes hacer uso de las plantillas utilizando un `MessagePromptTemplate`. Puedes construir un `ChatPromptTemplate` a partir de uno o más `MessagePromptTemplates`. Puedes utilizar el `format_prompt` de `ChatPromptTemplate` - esto devuelve un `PromptValue`, que puede convertir en una cadena o en un objeto Message, dependiendo de si desea utilizar el valor formateado como entrada a un modelo llm o chat.\n",
    "\n",
    "Para mayor comodidad, existe un método `from_template` expuesto en la plantilla. Si utilizas esta plantilla, esto es lo que parecería"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template=\"You are a helpful assistant that translates {input_language} to {output_language}.\"\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
    "human_template=\"{text}\"\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
    "\n",
    "# obten una finalización de chat a partir de los mensajes formateados\n",
    "chat(chat_prompt.format_prompt(input_language=\"English\", output_language=\"French\", text=\"I love programming.\").to_messages())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si quisieras construir el MessagePromptTemplate más directamente, podrías crear un PromptTemplate fuera y luego pasarlo, ej:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=PromptTemplate(\n",
    "    template=\"You are a helpful assistant that translates {input_language} to {output_language}.\",\n",
    "    input_variables=[\"input_language\", \"output_language\"],\n",
    ")\n",
    "system_message_prompt = SystemMessagePromptTemplate(prompt=prompt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLMChain\n",
    "\n",
    "Puede utilizar la LLMChain existente de una forma muy similar a la anterior: proporcione un indicador y un modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = LLMChain(llm=chat, prompt=chat_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.run(input_language=\"English\", output_language=\"French\", text=\"I love programming.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming\n",
    "\n",
    "`ChatOpenAI` soporta streaming a través del manejo de callbacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "chat = ChatOpenAI(streaming=True, callbacks=[StreamingStdOutCallbackHandler()], temperature=0)\n",
    "resp = chat([HumanMessage(content=\"Write me a song about sparkling water.\")])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guías prácticas\n",
    "\n",
    "Todos los ejemplos aquí expuestos abordan ciertas guías \"prácticas\" para trabajar con modelos de chat\n",
    "\n",
    ">Todas las guías aquí mencionadas son en inglés, de la documentación oficial de Langchain.\n",
    ">\n",
    ">Si veo apoyo en el contenido y ⭐ el repo de Github iré trayendo \"How To's\" en castellano\n",
    "\n",
    "\n",
    "- [How to use few shot examples](https://python.langchain.com/en/latest/modules/models/chat/examples/few_shot_examples.html)\n",
    "- [How to stream responses](https://python.langchain.com/en/latest/modules/models/chat/examples/streaming.html)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integraciones\n",
    "\n",
    "Todos estos ejemplos muestran cómo integrarse con distintos modelos de chat.\n",
    "\n",
    "- [Anthropic](https://python.langchain.com/en/latest/modules/models/chat/integrations/anthropic.html)\n",
    "- [Azure](https://python.langchain.com/en/latest/modules/models/chat/integrations/azure_chat_openai.html)\n",
    "- [Google Vertex AI PaLM](https://python.langchain.com/en/latest/modules/models/chat/integrations/google_vertex_ai_palm.html)\n",
    "- [OpenAI](https://python.langchain.com/en/latest/modules/models/chat/integrations/openai.html)\n",
    "- [PromptLayer ChatOpenAI](https://python.langchain.com/en/latest/modules/models/chat/integrations/promptlayer_chatopenai.html)\n",
    "\n",
    "___"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos de incrustación de texto (Text Embedding Models)\n",
    "\n",
    "Esta parte de la documentación oficial explica cómo utilizar la clase Embedding en LangChain.\n",
    "\n",
    "La clase Embedding es una clase diseñada para interactuar con embeddings. Hay muchos proveedores de Embedding (OpenAI, Cohere, Hugging Face, etc) - esta clase está diseñada para proporcionar una interfaz estándar para todos ellos.\n",
    "\n",
    "Las incrustaciones crean una representación vectorial de un fragmento de texto. Esto es útil porque significa que podemos pensar en el texto en el espacio vectorial, y hacer cosas como la búsqueda semántica donde buscamos piezas de texto que son más similares en el espacio vectorial.\n",
    "\n",
    "La clase base Embedding en LangChain expone dos métodos: embed_documents y embed_query. La mayor diferencia es que estos dos métodos tienen interfaces diferentes: uno trabaja sobre múltiples documentos, mientras que el otro trabaja sobre un único documento. Además de esto, otra razón para tenerlos como dos métodos separados es que algunos proveedores de incrustación tienen diferentes métodos de incrustación para documentos (para ser buscados) vs consultas (la consulta de búsqueda en sí).\n",
    "\n",
    "Existen las siguientes integraciones para incrustaciones de texto.\n",
    "\n",
    "- [Aleph Alpha]()\n",
    "- [Amazon Bedrock]()\n",
    "- [Azure OpenAI]()\n",
    "- [Cohere]()\n",
    "- [Elasticsearch]()\n",
    "- [Fake Embeddings]()\n",
    "- [Google Vertex AI PaLM]()\n",
    "- [Hugging Face Hub]()\n",
    "- [HuggingFace Instruct]()\n",
    "- [Jina]()\n",
    "- [Llama-cpp]()\n",
    "- [MiniMax]()\n",
    "- [ModelScope]()\n",
    "- [MosaicML]()\n",
    "- [OpenAI]()\n",
    "- [SageMaker Endpoint]()\n",
    "- [Self Hosted Embeddings]()\n",
    "- [Sentence Transformers]()\n",
    "- [Tensorflow Hub]()\n",
    "\n",
    "___"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recursos adicionales\n",
    "\n",
    "- [Documentación Langchain](https://python.langchain.com/en/latest/modules/models.html): https://python.langchain.com/en/latest/modules/models.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
