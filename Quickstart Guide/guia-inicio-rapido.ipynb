{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "907ba62a",
   "metadata": {},
   "source": [
    "# Guía de inicio rápido"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4a58bea8",
   "metadata": {},
   "source": [
    "Este tutorial le brindará una visión general y concisa sobre cómo construir una aplicación de modelo de lenguaje de extremo a extremo utilizando LangChain.\n",
    "\n",
    ">Docs: https://python.langchain.com/en/latest/getting_started/getting_started.html"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6c2cb957",
   "metadata": {},
   "source": [
    "**LangChain** es una biblioteca Python, creada por **Harrison Chase**, de código abierto diseñada específicamente para proporcionar a los desarrolladores las herramientas necesarias para crear aplicaciones basadas en grandes modelos lingüísticos (LLM) de manera eficiente y profesional.\n",
    "\n",
    "Con LangChain, los desarrolladores pueden conectarse fácilmente a diversas fuentes de datos y de cálculo, permitiéndoles crear aplicaciones que realicen tareas de procesamiento del lenguaje natural (PLN) en fuentes de datos específicas de dominio, repositorios privados y mucho más.\n",
    "\n",
    "Es un marco para la creación de agentes capaces de razonar y encadenar tareas. Brindando a los desarrolladores la capacidad de crear agentes inteligentes capaces de razonar sobre problemas complejos y dividirlos en subtareas más pequeñas. Con LangChain, es posible introducir contexto y memoria en las finalizaciones al crear pasos intermedios y encadenar comandos."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ba602cab",
   "metadata": {},
   "source": [
    "## Instalación\n",
    "Para empezar, instala LangChain con el siguiente comando:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68ef425b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (0.0.187)\n",
      "Requirement already satisfied: PyYAML>=5.4.1 in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (from langchain) (6.0)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (from langchain) (1.24.3)\n",
      "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (from langchain) (0.5.7)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (from langchain) (2.0.15)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (from langchain) (4.0.2)\n",
      "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (from langchain) (2.8.4)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (from langchain) (8.2.2)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (from langchain) (3.8.4)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: pydantic<2,>=1 in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (from langchain) (1.10.8)\n",
      "Requirement already satisfied: openapi-schema-pydantic<2.0,>=1.2 in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (from langchain) (1.2.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.1.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.3.0 in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (3.19.0)\n",
      "Requirement already satisfied: marshmallow-enum<2.0.0,>=1.5.1 in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (1.5.1)\n",
      "Requirement already satisfied: typing-inspect>=0.4.0 in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (0.9.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (from pydantic<2,>=1->langchain) (4.6.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (from requests<3,>=2->langchain) (2023.5.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (from requests<3,>=2->langchain) (2.0.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (from requests<3,>=2->langchain) (3.4)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.2)\n",
      "Requirement already satisfied: packaging>=17.0 in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (23.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (1.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install langchain"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f13c4d57",
   "metadata": {},
   "source": [
    "## Configuración del entorno\n",
    "Usar LangChain normalmente requerirá integraciones con uno o más proveedores de modelos, almacenes de datos, apis, etc.\n",
    "Para este ejemplo, vamos a utilizar las APIs de **OpenAI**, por lo que primero tendremos que instalar su SDK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "283dec4b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (0.27.7)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (from openai) (3.8.4)\n",
      "Requirement already satisfied: requests>=2.20 in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (from openai) (2.31.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (from openai) (4.65.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (from requests>=2.20->openai) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (from requests>=2.20->openai) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (from requests>=2.20->openai) (2023.5.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (from requests>=2.20->openai) (2.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (from aiohttp->openai) (1.9.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (from aiohttp->openai) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (from aiohttp->openai) (4.0.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (from aiohttp->openai) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (from aiohttp->openai) (1.3.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (from aiohttp->openai) (6.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (from tqdm->openai) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install openai"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "40e78a71",
   "metadata": {},
   "source": [
    "A continuación, tendremos que establecer la variable de entorno en nuestro archivo .env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4190bc17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-dotenv\n",
      "  Using cached python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
      "Installing collected packages: python-dotenv\n",
      "Successfully installed python-dotenv-1.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8add50f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "openai_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9d40bbf6",
   "metadata": {},
   "source": [
    "## Creación de una aplicación de modelos lingüísticos\n",
    "Ahora que hemos instalado LangChain y configurado nuestro entorno, podemos empezar a construir nuestra aplicación de modelo de lenguaje.\n",
    "\n",
    "LangChain proporciona muchos módulos que pueden ser utilizados para construir aplicaciones de modelos de lenguaje. Los módulos pueden combinarse para crear aplicaciones más complejas, o utilizarse individualmente para aplicaciones sencillas."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b6d8c664",
   "metadata": {},
   "source": [
    "### LLMs: Obtener predicciones de un modelo lingüístico\n",
    "El bloque de construcción más básico de LangChain es llamar a un LLM con algún input. Básicamente tener tu propio ChatGPT corriendo en tu terminal. \n",
    ">Consejo: Compara los precios del servicio ChatGPT Plus de OpenAI y sus precios para la API 😜\n",
    "\n",
    "En esta lección aprenderemos a interactuar con ChatGPT para generar texto. Muchas aplicaciones que están valoradas en millones de dólares como JasperAI, CopyAI... hacen lo mismo como parte de su negocio principal.\n",
    "\n",
    "Veamos un ejemplo sencillo de cómo hacerlo. Para ello, primero tenemos que importar el LLM wrapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94468c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4dec84a0",
   "metadata": {},
   "source": [
    "Podemos entonces inicializar el wrapper con cualquier argumento. En este ejemplo, probablemente queremos que las salidas sean standard, así que lo inicializaremos con una temperatura media/baja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7acfc0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0.3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f2eb1fa4",
   "metadata": {},
   "source": [
    "¡Ahora podemos llamarlo con algún input!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a99e791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Autonomous Agents Solutions Inc.\n"
     ]
    }
   ],
   "source": [
    "text = \"What would be a good company name for a company that makes Autonomous Agents?\"\n",
    "print(llm(text))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "030908e5",
   "metadata": {},
   "source": [
    "### Plantillas de Promps: Gestión de prompts para LLM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b012f594",
   "metadata": {},
   "source": [
    "Llamar a un LLM es un gran primer paso, pero es sólo el principio. Normalmente, cuando utilizas un LLM en una aplicación, no envías el input del usuario directamente al LLM. En su lugar, es probable que tomes el input del usuario, construyas un prompt y luego lo envíes al LLM.\n",
    "\n",
    "Por ejemplo, en el ejemplo anterior, el texto que pasamos estaba \"hardcoded\" para pedir el nombre de una empresa que fabrica agentes autónomos. En este servicio, lo que querríamos hacer es tomar sólo la entrada del usuario describiendo lo que hace la empresa, y luego formatear el prompt con esa información.\n",
    "\n",
    "Esto es fácil de hacer con LangChain.\n",
    "\n",
    "Primero definamos la plantilla del prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d5fe5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"product\"],\n",
    "    template=\"What is a good name for a company that makes {product}?\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8de273d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is a good name for a company that makes autonomous agent?\n"
     ]
    }
   ],
   "source": [
    "print(prompt.format(product=\"autonomous agent\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8f38a5b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Autonomous Agents, Inc.\n"
     ]
    }
   ],
   "source": [
    "print(llm(prompt.format(product=\"autonomous agent\")))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "79484db4",
   "metadata": {},
   "source": [
    "### Cadenas: Combina LLMs y prompts en flujos de trabajo de varios pasos."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7bb705f5",
   "metadata": {},
   "source": [
    "Hasta ahora, hemos trabajado con los PromptTemplate y LLM de base por sí solos. Pero, por supuesto, una aplicación real no es sólo una primitiva, sino más bien una combinación de ellas.\n",
    "\n",
    "Una cadena en LangChain se compone de enlaces, que pueden ser primitivas como LLMs u otras cadenas.\n",
    "\n",
    "El tipo más básico de cadena es una LLMChain, que consiste en un PromptTemplate y un LLM.\n",
    "\n",
    "Extendiendo el ejemplo anterior, podemos construir una LLMChain que tome la entrada del usuario, la formatee con un PromptTemplate, y luego pase la respuesta formateada a un LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a28f6df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0172b9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0.3)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"product\"],\n",
    "    template=\"What is a good name for a company that makes {product}?\",\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c00942b0",
   "metadata": {},
   "source": [
    "Ahora podemos crear una cadena muy simple que tomará la entrada del usuario, formateará el prompt con ella, y luego la enviará al LLM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d95591b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "83ae5c2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "CloudSoft Solutions.\n"
     ]
    }
   ],
   "source": [
    "print(chain.run(\"Software as a service\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dadcd871",
   "metadata": {},
   "source": [
    "Ya está. Esta es la primera cadena: una LLM Chain. Este es uno de los tipos más simples de cadenas, pero entender cómo funciona te preparará para trabajar con cadenas más complejas."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dd9ab0f7",
   "metadata": {},
   "source": [
    "### Agentes: Cadenas de llamadas dinámicas basadas en el input del usuario"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "09285d61",
   "metadata": {},
   "source": [
    "Hasta ahora, las cadenas que hemos visto se ejecutaban en un orden predeterminado.\n",
    "\n",
    "Los agentes ya no lo hacen: utilizan un LLM para determinar qué acciones realizar y en qué orden. Una acción puede consistir en utilizar una herramienta y observar su resultado, o en volver al usuario.\n",
    "\n",
    "Cuando se utilizan correctamente, los agentes pueden ser extremadamente potentes. En este tutorial, le mostramos cómo utilizar fácilmente los agentes a través de la API más simple y de más alto nivel."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "571fc83b",
   "metadata": {},
   "source": [
    "Para cargar agentes, debe comprender los siguientes conceptos:\n",
    "\n",
    "- **Herramienta**: Una función que realiza una tarea específica. Pueden ser cosas como: Búsqueda en Google, búsqueda en bases de datos, Python REPL, otras cadenas. La interfaz para una herramienta es actualmente una función que se espera que tenga una cadena como entrada, con una cadena como salida.\n",
    "\n",
    "- **LLM**: El modelo de lenguaje que alimenta al agente.\n",
    "\n",
    "- **Agente**: El agente a utilizar. Debe ser una cadena que haga referencia a una clase de agente de soporte. Dado que este cuaderno se centra en la API más simple y de más alto nivel, sólo cubre el uso de los agentes estándar soportados. Si desea implementar un agente personalizado, consulte la documentación https://python.langchain.com/en/latest/getting_started/getting_started.html#agents-dynamically-call-chains-based-on-user-input"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0e3a12c4",
   "metadata": {},
   "source": [
    "Para este ejemplo, también necesitarás instalar el paquete SerpAPI Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c934814c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install google-search-results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa92f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://serpapi.com/\n",
    "import os\n",
    "os.environ[\"SERPAPI_API_KEY\"] = \"...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e9a5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import AgentType\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6aead6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# En primer lugar, vamos a cargar el modelo lingüístico que vamos a utilizar para controlar el agente.\n",
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "# A continuación, vamos a cargar algunas herramientas a utilizar. Ten en cuenta que la herramienta `llm-math` utiliza un LLM, así que tenemos que pasarlo.\n",
    "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n",
    "\n",
    "\n",
    "# Por último, vamos a inicializar un agente con las herramientas, el modelo de lenguaje y el tipo de agente que queremos utilizar.\n",
    "agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc23ffd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ¡Ahora vamos a probarlo!\n",
    "agent.run(\"What was the high temperature in SF yesterday in Fahrenheit? What is that number raised to the .023 power?\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5353d05a",
   "metadata": {},
   "source": [
    "### Memoria: Añadir estado a cadenas y agentes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "76827448",
   "metadata": {},
   "source": [
    "Hasta ahora, todas las cadenas y agentes por los que hemos pasado han sido sin estado. Pero a menudo, es posible que quieras que una cadena o agente tenga algún concepto de \"memoria\" para que pueda recordar información sobre sus interacciones anteriores. El ejemplo más claro y sencillo de esto es el diseño de un chatbot: quieres que recuerde mensajes anteriores para que pueda utilizar su contexto para mantener una conversación mejor. Esto sería un tipo de \"memoria a corto plazo\". En el lado más complejo, se podría imaginar una cadena/agente que recordara piezas clave de información a lo largo del tiempo: esto sería una forma de \"memoria a largo plazo\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4bc9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import OpenAI, ConversationChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c2dfd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0)\n",
    "conversation = ConversationChain(llm=llm, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba101095",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = conversation.predict(input=\"Hi there!\")\n",
    "print(output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "75620a38",
   "metadata": {},
   "source": [
    "## Creación de una aplicación de modelos lingüísticos: Modelos de chat"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "95342e69",
   "metadata": {},
   "source": [
    "Del mismo modo, puedes utilizar modelos de chat en lugar de LLM. Los modelos de chat son una variación de los modelos de lenguaje. Aunque los modelos de chat utilizan modelos lingüísticos, la interfaz que exponen es un poco diferente: en lugar de exponer una API de \"entrada de texto, salida de texto\", exponen una interfaz en la que los \"mensajes de chat\" son las entradas y salidas.\n",
    "\n",
    "Las API de modelos de chat son bastante nuevas."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1dfa5f59",
   "metadata": {},
   "source": [
    "### Plantillas de mensajes de chat"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "73fabc37",
   "metadata": {},
   "source": [
    "De forma similar a los LLMs, puedes hacer uso de plantillas utilizando un MessagePromptTemplate.\n",
    "\n",
    "Puedes construir un ChatPromptTemplate a partir de uno o más MessagePromptTemplates. Puede utilizar el format_prompt de ChatPromptTemplate - esto devuelve un PromptValue, que puede convertir en una cadena o en un objeto Message, dependiendo de si desea utilizar el valor formateado como entrada a un modelo llm o chat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5719e835",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4321b8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatOpenAI(temperature=0)\n",
    "\n",
    "template = \"You are a helpful assistant that translates {input_language} to {output_language}.\"\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
    "human_template = \"{text}\"\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28fea27",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e75fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtén una finalización de chat a partir de los mensajes formateados\n",
    "chat(chat_prompt.format_prompt(input_language=\"English\", output_language=\"French\", text=\"I love programming.\").to_messages())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b86b0410",
   "metadata": {},
   "source": [
    "### Cadenas con modelos de chat\n",
    "La LLMChain comentada en la sección anterior también puede utilizarse con modelos de chat:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a75be03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain import LLMChain\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f73c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatOpenAI(temperature=0)\n",
    "\n",
    "template = \"You are a helpful assistant that translates {input_language} to {output_language}.\"\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
    "human_template = \"{text}\"\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac3ffba",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = LLMChain(llm=chat, prompt=chat_prompt)\n",
    "chain.run(input_language=\"English\", output_language=\"French\", text=\"I love programming.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ffde68eb",
   "metadata": {},
   "source": [
    "### Agentes con modelos de chat\n",
    "Los agentes también se pueden utilizar con modelos de chat, puedes inicializar uno utilizando AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION como tipo de agente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fa86bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import AgentType\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e620c37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# En primer lugar, vamos a cargar el modelo de lenguaje que vamos a utilizar para controlar el agente.\n",
    "chat = ChatOpenAI(temperature=0)\n",
    "\n",
    "# A continuación, vamos a cargar algunas herramientas a utilizar. Ten en cuenta que la herramienta `llm-math` utiliza un LLM, así que tenemos que pasarlo.\n",
    "llm = OpenAI(temperature=0)\n",
    "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n",
    "\n",
    "# Por último, vamos a inicializar un agente con las herramientas, el modelo de lenguaje y el tipo de agente que queremos utilizar.\n",
    "agent = initialize_agent(tools, chat, agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a369a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ¡Ahora vamos a probar!\n",
    "agent.run(\"Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0f6d1581",
   "metadata": {},
   "source": [
    "### Memoria: Añadir estado a cadenas y agentes\n",
    "Puedes utilizar la memoria con cadenas y agentes inicializados con modelos de chat. La principal diferencia entre esto y y l amemoria para LLMs es que en lugar de intentar condensar todos los mensajes anteriores en una cadena, podemos mantenerlos como su propio objeto de memoria único."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb96fc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate, \n",
    "    MessagesPlaceholder, \n",
    "    SystemMessagePromptTemplate, \n",
    "    HumanMessagePromptTemplate\n",
    ")\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be7eb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(\"The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"{input}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb5cf79",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0)\n",
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "conversation = ConversationChain(memory=memory, prompt=prompt, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7004166",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.predict(input=\"Hi there!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfee1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.predict(input=\"I'm doing well! Just having a conversation with an AI.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67bf5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.predict(input=\"Tell me about yourself.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece2a7ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34357e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ead4c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
